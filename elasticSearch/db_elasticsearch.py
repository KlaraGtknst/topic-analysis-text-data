import collections
import hashlib
from http.client import HTTPException
from itertools import repeat
from multiprocess import Pool
from elasticsearch import ApiError, ConflictError, Elasticsearch, NotFoundError
import base64
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from gensim.utils import simple_preprocess
# own modules
from text_embeddings.preprocessing.read_pdf import *
from user_interface.cli import *
from doc_images.pdf_matrix import *
from elasticSearch.queries.query_documents_tfidf import *
from text_embeddings.universal_sent_encoder_tensorFlow import *
from text_embeddings.hugging_face_sentence_transformer import *
from elasticSearch.queries.query_database import *
from doc_images.PCA.PCA_image_clustering import *
from text_embeddings.TFIDF.preprocessing.TfidfTextPreprocessor import *
from text_embeddings.InferSent.infer_pretrained import *
from text_embeddings import save_models 
from constants import *

'''------initiate, fill and search in database-------
run this code by typing and altering the path:
    python3 db_elasticsearch.py -d '/Users/klara/Documents/Uni/bachelorarbeit/data/0/*.pdf' -D '/Users/klara/Documents/Uni/bachelorarbeit/images/images/'
'''

def init_db(client: Elasticsearch, num_dimensions: int, sim_docs_vocab_size: int, n_components: int):
    '''
    :param client: Elasticsearch client
    :param num_dimensions: number of dimensions of the embedding
    :return: None

    This function initializes the database by creating an index (i.e. the structure for an entry of type 'bahamas' database).
    The index contains the following fields:
    - doc2vec: a dense vector of num_dimensions dimensions. This is the vector numerical representation of the document. Its similarity is measured by cosine similarity.
    - text: the text of the document. The text is not tokenized, stemmed etc.
    - path: the path to the document on the local maschine.
    - image: the image of the document (i.e. information about the document layout). The image is encoded in base64 and has 500 dpi.

    cf. https://www.elastic.co/guide/en/elasticsearch/reference/current/binary.html for information about binary for images
    cf. https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html for information about dense vectors and similarity measurement types
    '''
    client.indices.create(index='bahamas', mappings={
        "properties": {
            "doc2vec": {
                "type": "dense_vector",
                "dims": num_dimensions,
                "index": True,
                "similarity": "cosine",
            },
            "google_univ_sent_encoding": {
                "type": "dense_vector",
                "dims": 512,
                "index": True,
                "similarity": "cosine",
            },
            "huggingface_sent_transformer": {
                "type": "dense_vector",
                "dims": 384,
                "index": True,
                "similarity": "cosine",
            },
            "sim_docs_tfidf": {
                "type": "dense_vector",
                "dims": sim_docs_vocab_size + 1, # last cell indicates all-zero-vector-representation
                "index": True,
                "similarity": "cosine",
            },
            "inferSent_AE": {
                "type": "dense_vector",
                "dims": 2048,
                "index": True,
                "similarity": "cosine",
            },
            "pca_image": {
                "type": "dense_vector",
                "dims": n_components,
            },
            "pca_kmeans_cluster": {
                "type": "byte",
            },
            "text": {
                "type": "text",
            },
            "path": {
                "type": "keyword",
            },
            "image": {
                "type": "binary",
            },
        },
    })

def insert_documents(src_paths: list, pca_df: pd.DataFrame, client: Elasticsearch, image_path: str = None, n_pools: int = 1, client_addr: str = CLIENT_ADDR, model_names: list = MODEL_NAMES):
    '''
    :param src_paths: path to the documents to be inserted into the database
    :param doc2vec_model: Doc2Vec model
    :param client: Elasticsearch client
    :param google_model: google universal sentence encoder model
    :param huggingface_model: huggingface sentence transformer model
    :param tfidf_model: tfidf model
    :param find_doc_tfidf_vectorization: document-term matrix of the tfidf embedding to find a document from the corpus
    :param pca_df: dataframe of the pca weights, clusters generated by KMeans and the corresponding image paths as index
    :param inferEncoder: encoder from trained autoencoder model to reduce dimension of InferSent embedding
    :param inferSent_model: trained InferSent model
    :param image_path: path to the images of the documents to be inserted into the database; if not set, assumes the images are in the same folder as the documents.
    :return: None

    This function inserts the documents into the database 'bahamas'. The documents are inserted as follows:
    - doc2vec: the embedding is inferred from the document text using the trained Doc2Vec model.
        The text is preprocessed using the gensim function 'simple_preprocess', which returns a list of tokens, i.e. unicode strings,
        which are lowercase and tokenized. (cf. https://tedboy.github.io/nlps/generated/generated/gensim.utils.simple_preprocess.html).
    - text: the text of the document. The text is not tokenized, stemmed etc.
    - path: the path to the document on the local machine.
    - image: the image of the document (i.e. information about the document layout). The image is encoded in base64 and has 500 dpi.
    
    The documents receive an id which is the name of the document (i.e. the name of the pdf file without the extension).

    cf. https://stackoverflow.com/questions/8908287/why-do-i-need-b-to-encode-a-string-with-base64 for information about base64 encoding
    cf. https://www.codespeedy.com/convert-image-to-base64-string-in-python/ for information about converting images to base64
    '''
    image_path = image_path if image_path else (src_paths.split('data/0/')[0] + 'images/images/')
    print('start multiprocessing'if n_pools > 1 else 'start single processing')
    #models = get_models(src_paths) if n_pools == 1 else None
    models = None
    if n_pools == 1:    # single processing
        for src_path in src_paths:
            insert_document(src_path, pca_df, image_path, client_addr=client_addr, models=models, client=client, model_names=model_names)
    else: # multiprocessing, TODO: does not work yet, bc of models is not pickable
        with Pool(n_pools) as p: # number of cpus n_pools
            p.starmap(insert_document, list(map(lambda src_path:[src_path, pca_df, image_path, client_addr, models], src_paths)))

def get_models(src_paths: list, model_names: list = MODEL_NAMES):
    models = {}
    if 'infer' in model_names and (not 'ae' in model_names):    # needs AE for embedding
        model_names = model_names + ['ae']
    for model_name in model_names:
        try: # model exists
            model = save_models.load_model(model_name)
            models[model_name] = model
        except: # model does not exist, create and save it
            model = save_models.train_model(model_name, src_paths)
            models[model_name] = model
            save_models.save_model(model, model_name)
    return models

def insert_document(src_path, pca_df, image_path, models, client_addr=CLIENT_ADDR, client: Elasticsearch=None, model_names: list = MODEL_NAMES):
        client = client if client else Elasticsearch(client_addr)
        path = src_path
        models = models if models else get_models(src_path, model_names if model_names else None)
        try:
            try:
                text = pdf_to_str(path)
            except:
                # missing EOF marker in pdf
                print(f'EOF marker missing in {path}.')
                return
            
            BLOCK_SIZE = 65536 # The size of each read from the file
            file_hash = hashlib.sha256() # Create the hash object, can use something other than `.sha256()` if you wish
            with open(path, 'rb') as f: # Open the file to read it's bytes
                fb = f.read(BLOCK_SIZE) # Read from the file. Take in the amount declared above
                while len(fb) > 0: # While there is still data being read from the file
                    file_hash.update(fb) # Update the hash
                    fb = f.read(BLOCK_SIZE)
            id = file_hash.hexdigest()
            f.close()

            try:
                get_doc_meta_data(client, doc_id=id)    # document already in database
                return
            except NotFoundError:   # insert new document
                # TODO: batch aus nicht inserted documents?
                if 'tfidf' in model_names:
                    # TFIDF embedding
                    tfidf_emb = models['tfidf'].transform([text]).todense()
                    flag = np.array(1 if np.array([entry  == 0 for entry in tfidf_emb]).all() else 0).reshape(len(tfidf_emb),1)
                    flag_matrix = np.append(tfidf_emb, flag, axis=1)

                image = image_path + path.split('/')[-1].split('.')[0]  + '.png'
                try:
                    with open(image, "rb") as img_file:
                        b64_image = base64.b64encode(img_file.read())
                except FileNotFoundError:
                    # bc i did not copy all images from cluster to local machine
                    # dummy (black) picture to avoid not inserting the document into the database
                    b64_image = base64.b64encode(np.zeros([100,100,3],dtype=np.uint8)) # TODO: or insert None

                pca_df_row = pca_df.loc[pca_df.index == image] if image in list(pca_df.index) else None

                if 'infer' in model_names:
                    # InferSent embedding
                    inferSent_embedding = models['infer'].encode([text, text], tokenize=True)
                    compressed_infersent_embedding = models['ae'].predict(x=inferSent_embedding)[0]

                try:    # TODO: alle models auf einmal im Speicher Problem?
                    if len(models.keys()) >= 6: # all embeddings
                        client.create(index='bahamas', id=id, document={ 
                            "doc2vec": models['doc2vec'].infer_vector(simple_preprocess(pdf_to_str(path))),
                            "sim_docs_tfidf": np.ravel(np.array(flag_matrix)),
                            "google_univ_sent_encoding": embed([text], models['universal']).numpy().tolist()[0],
                            "huggingface_sent_transformer": models['hugging'].encode(text),
                            "inferSent_AE": compressed_infersent_embedding,
                            "pca_image": pca_df_row['pca_weights'].item() if pca_df_row is not None else None,
                            "pca_kmeans_cluster": pca_df_row['cluster'] if pca_df_row is not None else None,
                            "text": text,
                            "path": path,
                            "image": b64_image.decode('ASCII')#str(b64_image) # TODO: statt str... b64_image.decode('ASCII'),
                        })
                    else:   # not all embeddings
                        name_to_field = {
                                        'doc2vec': {"doc2vec": models['doc2vec'].infer_vector(simple_preprocess(pdf_to_str(path))) if 'doc2vec' in model_names else None}, 
                                        'universal': {"google_univ_sent_encoding": embed([text], models['universal']).numpy().tolist()[0] if 'universal' in model_names else None},
                                        'hugging': {"huggingface_sent_transformer": models['hugging'].encode(text) if 'hugging' in model_names else None}, 
                                        'infer': {"inferSent_AE": compressed_infersent_embedding if 'infer' in model_names else None},
                                        'tfidf': {"sim_docs_tfidf": np.ravel(np.array(flag_matrix)) if 'tfidf' in model_names else None},
                                        }
                        client.create(index='bahamas', id=id, document={ 
                            "pca_image": pca_df_row['pca_weights'].item() if pca_df_row is not None else None,  # TODO: rausnehmen
                            "pca_kmeans_cluster": pca_df_row['cluster'] if pca_df_row is not None else None,    # TODO: rausnehmen
                            "text": text,
                            "path": path,
                            "image": b64_image.decode('ASCII')#str(b64_image) # TODO: statt str... b64_image.decode('ASCII'),
                        })
                        for model_name in model_names:
                            if model_name in name_to_field.keys():
                                client.update(index='bahamas', id=id, refresh=True, body={"doc": name_to_field[model_name]})
                            else:   # ae has no solo embedding
                                continue
                except ApiError as err:
                    print('err1')
                    return
        except ConflictError as err:
            print('er2')
            return



def get_tagged_input_documents(src_paths: list, tokens_only: bool = False):
    '''
    :param src_paths: list of paths to the documents to be inserted into the database
    :param tokens_only: if True, only the tokens of the document are returned, else tagged tokens are returned
    :return: tagged tokens or only the tokens (depending on tokens_only)

    The gensim function 'simple_preprocess' converts a document into a list of tokens (cf. https://tedboy.github.io/nlps/generated/generated/gensim.utils.simple_preprocess.html).
    The resulting list of unicode strings is lowercased and tokenized.

    cf. https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py
    for the original code
    '''
    for i in range(len(src_paths)):
        path = src_paths[i]
        tokens = simple_preprocess(pdf_to_str(path))
        if tokens_only:
            yield tokens
        else:
            yield TaggedDocument(tokens, [i])
   



def assess_model(model: Doc2Vec, train_corpus: list):
    '''
    :param model: trained Doc2Vec model
    :param train_corpus: list of tagged documents
    :return: None

    This function assesses by:
    (1) infer a vector of a document from the training corpus
    (2) compare inferred vector with the vector of the document in the training corpus
    (3) return rank of the document based on self-similarity (i.e. the document itself should be ranked first)
    '''
    print('-'*100)
    ranks = []  # list of ranks the documents got when compared to themselves
    second_ranks = []   # list of similarities of the second most similar document
    for doc_id in range(len(train_corpus)):
        inferred_vector = model.infer_vector(train_corpus[doc_id].words)
        sims = model.dv.most_similar([inferred_vector], topn=len(model.dv)) # topn: number of most similar documents to be returned
        rank = [docid for docid, sim in sims].index(doc_id) # rank of the document via id
        ranks.append(rank)  # saves the rank of the document in terms of self-similarity
        second_ranks.append(sims[1])    # saves the similarity of the second most similar document

        print('Document ({}): «{}»\n'.format(doc_id, ' '.join(train_corpus[doc_id].words[:10])))
        print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\n' % model)
        for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:
            print(u'%s %s: «%s»\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words[:10])))
        print('-'*80)
    counter = collections.Counter(ranks)
    print(f'{counter[0]} documents are most self-similar to themselves.\n{counter[1]} documents were not ranked first.')
    

def infer_embedding_for_single_document(model: Doc2Vec, text: str):
    '''
    :param model: trained Doc2Vec model
    :param text: text of the document to be embedded
    :return: embedding vector of the document

    This function infers the embedding vector of a document.
    '''
    vector = model.infer_vector(simple_preprocess(text))
    return vector

def tfidf_aux(src_paths: list) -> tuple:
    '''
    :param src_paths: paths to the documents to be inserted into the database
    :return: document-term matrix with a all-zero-flag-cell at the end and the trained tfidf vectorizer model
    '''
    docs = get_docs_from_file_paths(src_paths)
    # usage of custom preprocessor
    sim_docs_tfidf = TfidfVectorizer(input='content', preprocessor=TfidfTextPreprocessor().transform, min_df=3, max_df=int(len(docs)*0.07))
    sim_docs_document_term_matrix = sim_docs_tfidf.fit_transform(docs).todense()

    # add flags, which indicate if the document is represented by an all zero vector
    flags = np.array([1 if np.array([entry  == 0 for entry in sim_docs_document_term_matrix[i]]).all() else 0 for i in range(len(sim_docs_document_term_matrix))]).reshape(len(sim_docs_document_term_matrix),1)
    flag_matrix = np.append(sim_docs_document_term_matrix, flags, axis=1)

    return flag_matrix, sim_docs_tfidf


def show_best_search_results(scores, src_paths, image_src_path=None):
    # create image matrix of 9 most similar images for query image
    image_paths = [image_src_path + id.split('.')[0]  + '.png' if image_src_path else src_paths.split('.')[0] + '.png' for id in scores.values()]
    create_image_matrix(input_files=image_paths, dim=3, output_path=None)


def init_db_aux(src_paths, image_src_path, client_addr=CLIENT_ADDR, n_pools=1, model_names: list = MODEL_NAMES):
    '''
    everything that happens in the main function to fill the database.
    '''
    NUM_DIMENSIONS = 55
    NUM_COMPONENTS = 13

    print('-' * 80)

    sim_docs_vocab_size = len(save_models.load_model("tfidf").vocabulary_.values())
    #len(models['tfidf'].vocabulary_.values())

    # Create the client instance
    client = Elasticsearch(client_addr)
    print('finished creating client')

    # delete old index and create new one
    client.options(ignore_status=[400,404]).indices.delete(index='bahamas')
    init_db(client, num_dimensions=NUM_DIMENSIONS, sim_docs_vocab_size=sim_docs_vocab_size, n_components=NUM_COMPONENTS)
    print('finished deleting old and creating new index')

    # Eigendocs (PCA) + OPTICS clustering
    pca_cluster_df = get_eigendocs_OPTICS_df(image_src_path, n_components=NUM_COMPONENTS)
    print('finished getting pca-OPTICS cluster df')

    # insert documents into database
    print(f'start inserting {len(src_paths)} documents')
    insert_documents(src_paths, client=client, image_path=image_src_path, n_pools=n_pools, pca_df=pca_cluster_df, model_names=model_names)
    print('finished inserting documents')

    # alternatively, use AsyncElasticsearch or time.sleep(1)
    client.indices.refresh(index="bahamas")

    # properties in db
    print(client.indices.get_mapping(index='bahamas'))

    # number of documents in database
    client.indices.refresh(index='bahamas')
    resp = client.count(index='bahamas')
    print('number of documents in database: ', resp['count'])

def main(src_paths, image_src_path, client_addr=CLIENT_ADDR, n_pools=1, model_names: list = MODEL_NAMES):
    init_db_aux(src_paths, image_src_path, client_addr=client_addr, n_pools=n_pools, model_names=model_names)
    

if __name__ == '__main__':
    args = arguments()

    file_paths = get_input_filepath(args)
    out_file = get_filepath(args, option='output')
    image_src_path = get_filepath(args, option='image')
    model_names = get_model_names(args)

    init_db_aux(src_paths=file_paths, image_src_path=image_src_path, client_addr=CLIENT_ADDR, model_names=model_names)